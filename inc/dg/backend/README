contains engines for the container free numerical algorithms and test programs as
well as additional utility functions

Performance hints
===========================================
FIRST: (Small things to do)
1. Implement transposition of mpi interpolation for dz.h (explanation further below)
   - transpose interpolation matrices locally
   - first apply collective.gather()
   - then do a gemv on the buffer 

2. Implement an mpi version of create::dz and the application of it in mpi_matrix.h (if needed in general_elliptic.h)

THIRD: (road to MPI + X, where X is all we already have, i.e. OpenMP, MIC, or GPU)
The general idea is to separate global communication from local parallelization and thus 
readily reuse the existing, optimized library for the local part

2. implement a general sparse mpi matrix format (unifying the existing mpi_matrix and the mpi-interpolation )
The idea is that the mpi matrix takes care of the communication and then defers the actual computation 
to an existing local matrix format.

    There are two approaches to distribute a global sparse matrix to nodes: row distributed (RD) or 
    column distributed (CD). In the first each node gets the same rows as the vector it holds, in the
    second it gets the same columns as the vector it holds.
    IDEA how to then implement gemv(input, output) algorithm:
    RD: 1. create a local send buffer and locally gather values from input vector (c) into a send buffer (am Besten nach PID geordnet, note that a given value can be sent to several processes -> that's why it's a gather)
        2. globally scatter these values into recv buffer (b)  (The first two points are a global gather)
        3. then apply the local matrix to that buffer and store result in output vector (a)

    CD: 1. apply the local matrix on the input vector and store result in a local send buffer,  (a)
        2. globally scatter the values in this buffer to a recv buffer (b) (every value in the result belongs to exactly one line/process)
        3. then permute and reduce the recv buffer on double indices and store result in output vector (c) 
    
        Transposition is easy: if a RD matrix is transposed you get a CD matrix, also transpose the Collective object (swap scatter and gather maps)

    Note that there are three types of indices that you need to consider: 
    1) the global vector index is the index of an element if there was only one vector that lay contiguously in memory. 
    2) the local vector index is the index of the local chunk a process has. 
    3) the buffer index is the index into the communication buffer. 
    
    The matrix class needs three private data members for the three steps: 
    a) The local matrix, (using buffer indices, the type should probably be a template parameter)
    b) A Collective object which holds a communication pattern to take care of the global scatter operations,  and the communication between device and host e.g.
    c) A local index map, to map the local buffer indices to local row/col indices
    
    IDEA: how to create such a matrix:
    If you can, manually create (a), (b), (c) and construct the matrix. 
    The current mpi matrix is a row distributed matrix and the communication 
    is done by manual MPI_SendRecv calls. This has to (maybe even should) be replaced by a Collective object, which
    internally uses MPI_Alltoallv (If need be, the MPI_Alltoallv might be replaced by MPI_Neighbor_alltoallv in MPI 3)


    If you cannot or you don't want to:
    1. create local sparse matrices holding global row/col indices and distribute row-wise or column-wise
    ( there needs to be a map or function that can map global indices to local ones and the corresponding PID)
    2. unique_copy global row (CD)/ col (RD) indices into buffer (no mistake here :-))
    3. map global indices in this buffer to local indices and PIDs
    4. in the local matrix replace row( CD)/ col(RD) indices by the corresponding buffer indices (find & replace) -> (a)
         (this might be slow) 
    5. with the PIDs construct Collective (b), then globally scatter the indices -> (c)

